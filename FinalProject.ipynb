{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0542f023",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "https://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 10)\n",
      "Index(['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id',\n",
      "       'timestamp', 'helpful_vote', 'verified_purchase'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_gz_json(path, max_records=None):\n",
    "    data = []\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            data.append(json.loads(line))\n",
    "            if max_records and (i + 1) >= max_records:\n",
    "                break\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load reviews\n",
    "reviews_df = load_gz_json('data/Electronics.jsonl.gz', max_records=1000000)\n",
    "print(reviews_df.shape)\n",
    "print(reviews_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba71f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 16)\n",
      "Index(['main_category', 'title', 'average_rating', 'rating_number', 'features',\n",
      "       'description', 'price', 'images', 'videos', 'store', 'categories',\n",
      "       'details', 'parent_asin', 'bought_together', 'subtitle', 'author'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "meta_df = load_gz_json('data/meta_Electronics.jsonl.gz', max_records=1000000)\n",
    "print(meta_df.shape)\n",
    "print(meta_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62874c73",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f5bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gauravkh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gauravkh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\gauravkh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\gauravkh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gauravkh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gauravkh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\gauravkh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gauravkh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gauravkh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gauravkh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "reviews_df['tokens'] = reviews_df['text'].fillna('').apply(preprocess)\n",
    "reviews_df['clean_text'] = reviews_df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab9aeb",
   "metadata": {},
   "source": [
    "# Sentiment Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c07bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_label(rating):\n",
    "    if rating >= 4:\n",
    "        return 1   # positive\n",
    "    elif rating <= 2:\n",
    "        return 0   # negative\n",
    "    else:\n",
    "        return None  # neutral / skip\n",
    "\n",
    "reviews_df['label'] = reviews_df['rating'].apply(get_sentiment_label)\n",
    "reviews_df = reviews_df.dropna(subset=['label'])  # remove neutral or missing\n",
    "reviews_df['label'] = reviews_df['label'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3abea0",
   "metadata": {},
   "source": [
    "# Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f80bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "X = reviews_df['clean_text'].values\n",
    "y = reviews_df['label'].values\n",
    "\n",
    "accuracies = []\n",
    "fold = 1\n",
    "\n",
    "def run_log_reg(X, y, vectorizer, model, k=5):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        preds = model.predict(X_test_vec)\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        scores.append((acc, f1))\n",
    "        print(f\"Fold {fold + 1}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
    "        print(classification_report(y_test, preds, digits=3))\n",
    "\n",
    "    avg_acc = np.mean([s[0] for s in scores])\n",
    "    print(\"Avg Accuracy:\", avg_acc)\n",
    "    print(\"Avg F1 Score:\", np.mean([s[1] for s in scores]))\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531c3af",
   "metadata": {},
   "source": [
    "# 10 different configs for ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84463a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Configuration 1: {'max_features': 10000}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9310, F1=0.9603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.836     0.661     0.739     27347\n",
      "           1      0.944     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.890     0.819     0.849    185678\n",
      "weighted avg      0.928     0.931     0.928    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.659     0.735     27347\n",
      "           1      0.943     0.977     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.888     0.818     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9301, F1=0.9597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.659     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.818     0.847    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9300, F1=0.9597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.659     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.818     0.847    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9303890176612513\n",
      "Avg F1 Score: 0.959904051369562\n",
      "Running Configuration 2: {'max_features': 5000}\n",
      "Fold 1: Accuracy=0.9293, F1=0.9593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.830     0.654     0.732     27348\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.929    185678\n",
      "   macro avg      0.886     0.815     0.845    185678\n",
      "weighted avg      0.926     0.929     0.926    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9300, F1=0.9596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.829     0.660     0.735     27347\n",
      "           1      0.943     0.977     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.886     0.818     0.847    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9290, F1=0.9591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.829     0.652     0.730     27347\n",
      "           1      0.942     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.929    185678\n",
      "   macro avg      0.886     0.815     0.845    185678\n",
      "weighted avg      0.925     0.929     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9287, F1=0.9590\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.648     0.728     27347\n",
      "           1      0.941     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.929    185677\n",
      "   macro avg      0.886     0.813     0.844    185677\n",
      "weighted avg      0.925     0.929     0.925    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9290, F1=0.9591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.829     0.653     0.731     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.929    185677\n",
      "   macro avg      0.886     0.815     0.845    185677\n",
      "weighted avg      0.926     0.929     0.925    185677\n",
      "\n",
      "Avg Accuracy: 0.9292106310824971\n",
      "Avg F1 Score: 0.9592389478726107\n",
      "Running Configuration 3: {'max_features': 100000}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.944     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9303, F1=0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.835     0.657     0.735     27347\n",
      "           1      0.943     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.889     0.817     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.645     0.726     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.928    185678\n",
      "   macro avg      0.886     0.811     0.842    185678\n",
      "weighted avg      0.925     0.928     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9296, F1=0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.654     0.732     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.815     0.846    185677\n",
      "weighted avg      0.926     0.930     0.926    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.658     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.888     0.818     0.848    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9298159822740306\n",
      "Avg F1 Score: 0.9595915944994615\n",
      "Running Configuration 4: {'max_features': 500000}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.944     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9303, F1=0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.835     0.657     0.735     27347\n",
      "           1      0.943     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.889     0.817     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.645     0.726     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.928    185678\n",
      "   macro avg      0.886     0.811     0.842    185678\n",
      "weighted avg      0.925     0.928     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9296, F1=0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.654     0.732     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.815     0.846    185677\n",
      "weighted avg      0.926     0.930     0.926    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.658     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.888     0.818     0.848    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9298159822740306\n",
      "Avg F1 Score: 0.9595915944994615\n",
      "Running Configuration 5: {'ngram_range': (1, 2)}\n",
      "Fold 1: Accuracy=0.9380, F1=0.9643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.865     0.686     0.765     27348\n",
      "           1      0.948     0.981     0.964    158330\n",
      "\n",
      "    accuracy                          0.938    185678\n",
      "   macro avg      0.906     0.834     0.865    185678\n",
      "weighted avg      0.935     0.938     0.935    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9380, F1=0.9643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.868     0.682     0.764     27347\n",
      "           1      0.947     0.982     0.964    158331\n",
      "\n",
      "    accuracy                          0.938    185678\n",
      "   macro avg      0.908     0.832     0.864    185678\n",
      "weighted avg      0.935     0.938     0.935    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9369, F1=0.9637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.865     0.677     0.759     27347\n",
      "           1      0.946     0.982     0.964    158331\n",
      "\n",
      "    accuracy                          0.937    185678\n",
      "   macro avg      0.906     0.829     0.862    185678\n",
      "weighted avg      0.934     0.937     0.934    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9375, F1=0.9640\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.867     0.680     0.762     27347\n",
      "           1      0.947     0.982     0.964    158330\n",
      "\n",
      "    accuracy                          0.938    185677\n",
      "   macro avg      0.907     0.831     0.863    185677\n",
      "weighted avg      0.935     0.938     0.934    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9371, F1=0.9638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.863     0.681     0.761     27347\n",
      "           1      0.947     0.981     0.964    158330\n",
      "\n",
      "    accuracy                          0.937    185677\n",
      "   macro avg      0.905     0.831     0.863    185677\n",
      "weighted avg      0.934     0.937     0.934    185677\n",
      "\n",
      "Avg Accuracy: 0.9374830347096083\n",
      "Avg F1 Score: 0.9640047270500816\n",
      "Running Configuration 6: {'use_idf': False}\n",
      "Fold 1: Accuracy=0.9294, F1=0.9594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.652     0.731     27348\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.929    185678\n",
      "   macro avg      0.887     0.815     0.845    185678\n",
      "weighted avg      0.926     0.929     0.926    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9290, F1=0.9591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.650     0.730     27347\n",
      "           1      0.942     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.929    185678\n",
      "   macro avg      0.886     0.814     0.844    185678\n",
      "weighted avg      0.925     0.929     0.925    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9288, F1=0.9591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.647     0.728     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.929    185678\n",
      "   macro avg      0.887     0.812     0.844    185678\n",
      "weighted avg      0.925     0.929     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.644     0.726     27347\n",
      "           1      0.941     0.978     0.959    158330\n",
      "\n",
      "    accuracy                          0.928    185677\n",
      "   macro avg      0.887     0.811     0.842    185677\n",
      "weighted avg      0.925     0.928     0.925    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9279, F1=0.9585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.827     0.646     0.725     27347\n",
      "           1      0.941     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.928    185677\n",
      "   macro avg      0.884     0.811     0.842    185677\n",
      "weighted avg      0.924     0.928     0.924    185677\n",
      "\n",
      "Avg Accuracy: 0.9287259194440898\n",
      "Avg F1 Score: 0.9589879708558152\n",
      "Running Configuration 7: {'stop_words': None}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.944     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9303, F1=0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.835     0.657     0.735     27347\n",
      "           1      0.943     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.889     0.817     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.645     0.726     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.928    185678\n",
      "   macro avg      0.886     0.811     0.842    185678\n",
      "weighted avg      0.925     0.928     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9296, F1=0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.654     0.732     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.815     0.846    185677\n",
      "weighted avg      0.926     0.930     0.926    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.658     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.888     0.818     0.848    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9298159822740306\n",
      "Avg F1 Score: 0.9595915944994615\n",
      "Running Configuration 8: {'lowercase': True}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.944     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9303, F1=0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.835     0.657     0.735     27347\n",
      "           1      0.943     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.889     0.817     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.645     0.726     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.928    185678\n",
      "   macro avg      0.886     0.811     0.842    185678\n",
      "weighted avg      0.925     0.928     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9296, F1=0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.654     0.732     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.815     0.846    185677\n",
      "weighted avg      0.926     0.930     0.926    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.658     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.888     0.818     0.848    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9298159822740306\n",
      "Avg F1 Score: 0.9595915944994615\n",
      "Running Configuration 9: {'lowercase': False}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.944     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9303, F1=0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.835     0.657     0.735     27347\n",
      "           1      0.943     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.889     0.817     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.645     0.726     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.928    185678\n",
      "   macro avg      0.886     0.811     0.842    185678\n",
      "weighted avg      0.925     0.928     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9296, F1=0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.654     0.732     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.815     0.846    185677\n",
      "weighted avg      0.926     0.930     0.926    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.658     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.888     0.818     0.848    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9298159822740306\n",
      "Avg F1 Score: 0.9595915944994615\n",
      "Running Configuration 10: {'max_df': 0.9}\n",
      "Fold 1: Accuracy=0.9306, F1=0.9600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.661     0.737     27348\n",
      "           1      0.944     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.931    185678\n",
      "   macro avg      0.888     0.819     0.849    185678\n",
      "weighted avg      0.927     0.931     0.927    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9303, F1=0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.835     0.657     0.735     27347\n",
      "           1      0.943     0.978     0.960    158331\n",
      "\n",
      "    accuracy                          0.930    185678\n",
      "   macro avg      0.889     0.817     0.848    185678\n",
      "weighted avg      0.927     0.930     0.927    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9284, F1=0.9588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.645     0.726     27347\n",
      "           1      0.941     0.977     0.959    158331\n",
      "\n",
      "    accuracy                          0.928    185678\n",
      "   macro avg      0.886     0.811     0.842    185678\n",
      "weighted avg      0.925     0.928     0.925    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9296, F1=0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.654     0.732     27347\n",
      "           1      0.942     0.977     0.959    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.887     0.815     0.846    185677\n",
      "weighted avg      0.926     0.930     0.926    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9302, F1=0.9598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.658     0.735     27347\n",
      "           1      0.943     0.977     0.960    158330\n",
      "\n",
      "    accuracy                          0.930    185677\n",
      "   macro avg      0.888     0.818     0.848    185677\n",
      "weighted avg      0.927     0.930     0.927    185677\n",
      "\n",
      "Avg Accuracy: 0.9298159822740306\n",
      "Avg F1 Score: 0.9595915944994615\n",
      "Running Configuration 11: {'max_features': 10000, 'ngram_range': (2, 2)}\n",
      "Fold 1: Accuracy=0.9065, F1=0.9471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.814     0.473     0.599     27348\n",
      "           1      0.915     0.981     0.947    158330\n",
      "\n",
      "    accuracy                          0.906    185678\n",
      "   macro avg      0.865     0.727     0.773    185678\n",
      "weighted avg      0.900     0.906     0.896    185678\n",
      "\n",
      "Fold 2: Accuracy=0.9064, F1=0.9470\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.811     0.475     0.599     27347\n",
      "           1      0.915     0.981     0.947    158331\n",
      "\n",
      "    accuracy                          0.906    185678\n",
      "   macro avg      0.863     0.728     0.773    185678\n",
      "weighted avg      0.900     0.906     0.896    185678\n",
      "\n",
      "Fold 3: Accuracy=0.9063, F1=0.9470\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.816     0.470     0.596     27347\n",
      "           1      0.915     0.982     0.947    158331\n",
      "\n",
      "    accuracy                          0.906    185678\n",
      "   macro avg      0.865     0.726     0.771    185678\n",
      "weighted avg      0.900     0.906     0.895    185678\n",
      "\n",
      "Fold 4: Accuracy=0.9065, F1=0.9470\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.808     0.479     0.601     27347\n",
      "           1      0.916     0.980     0.947    158330\n",
      "\n",
      "    accuracy                          0.906    185677\n",
      "   macro avg      0.862     0.730     0.774    185677\n",
      "weighted avg      0.900     0.906     0.896    185677\n",
      "\n",
      "Fold 5: Accuracy=0.9069, F1=0.9473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.812     0.479     0.602     27347\n",
      "           1      0.916     0.981     0.947    158330\n",
      "\n",
      "    accuracy                          0.907    185677\n",
      "   macro avg      0.864     0.730     0.775    185677\n",
      "weighted avg      0.901     0.907     0.896    185677\n",
      "\n",
      "Avg Accuracy: 0.9064916827496102\n",
      "Avg F1 Score: 0.9470686526789361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "configs = [\n",
    "    {\"max_features\": 10000},\n",
    "    {\"max_features\": 5000},\n",
    "    {\"max_features\": 100000},\n",
    "    {\"max_features\": 500000},\n",
    "    {\"ngram_range\": (1, 2)},\n",
    "    {\"use_idf\": False},   \n",
    "    {\"stop_words\": None}, \n",
    "    {\"lowercase\": True},\n",
    "    {\"lowercase\": False},\n",
    "    {\"max_df\": 0.9},\n",
    "    {\"max_features\": 10000, \"ngram_range\": (2, 2)},\n",
    "]\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    print(f\"Running Configuration {i+1}: {config}\")\n",
    "    vectorizer = TfidfVectorizer(**config)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    acc = run_log_reg(X, y, vectorizer, model)\n",
    "    if  acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9980637",
   "metadata": {},
   "source": [
    "# Extreme Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696b6fe",
   "metadata": {},
   "source": [
    "--This needs updating, doenst look right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72786bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work great\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work installed instruction provided broadband work great nothing\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work great ridiculously overpriced piece basic plastic\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work great darn small\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work great constantly dropping phone connecting general\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work great\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "work great etc begin fade away well color\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "would move jabra work great doesnt cut\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "glue work great lol\n",
      "\n",
      "True: 0, Pred: 1, Confidence: 1.00\n",
      "product snap back customer unit via fixed tab hinged tab fixed tab thick ca enter notch gps delivered useless ca used deterred shaved thousandth inch tab work great\n"
     ]
    }
   ],
   "source": [
    "# Transform X to TF-IDF features\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "\n",
    "best_model.fit(X_tfidf, y)\n",
    "preds = best_model.predict(X_tfidf)\n",
    "\n",
    "# Find the most confident wrong predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "errors = (preds != y)\n",
    "probs = best_model.predict_proba(X_tfidf)\n",
    "conf_scores = np.max(probs, axis=1)\n",
    "\n",
    "# Most confident incorrect predictions\n",
    "extreme_errors = np.argsort(-conf_scores[errors])[:10]\n",
    "\n",
    "for idx in np.where(errors)[0][extreme_errors]:\n",
    "    print(f\"\\nTrue: {y[idx]}, Pred: {preds[idx]}, Confidence: {conf_scores[idx]:.2f}\")\n",
    "    print(reviews_df.iloc[idx]['clean_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100fbe2",
   "metadata": {},
   "source": [
    "# RNN with Word2Vec and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35556a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "sentences = [tokens for tokens in reviews_df['tokens'] if len(tokens) > 0]\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size=4, min_count=5, workers=4, sg=0, epochs=5)\n",
    "\n",
    "vocab = w2v_model.wv.key_to_index.copy()\n",
    "original_vocab_size = len(vocab)\n",
    "embedding_dim = w2v_model.wv.vector_size\n",
    "vocab['<PAD>'] = original_vocab_size\n",
    "vocab['<UNK>'] = original_vocab_size + 1\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88b962a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader size: 11605, Test loader size: 2902\n"
     ]
    }
   ],
   "source": [
    "def text_to_sequence(tokens, vocab, max_length=200):\n",
    "    sequence = [vocab.get(token, vocab['<UNK>']) for token in tokens[:max_length]]\n",
    "    sequence += [vocab['<PAD>']] * (max_length - len(sequence))\n",
    "    return sequence\n",
    "\n",
    "def prepare_data_tensors(texts, labels, vocab, max_length=200):\n",
    "    sequences = [text_to_sequence(tokens, vocab, max_length) for tokens in texts]\n",
    "    X_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "def create_data_loader(X_tensor, y_tensor, batch_size=64, shuffle=True):\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "max_sequence_length = 200\n",
    "X_tokens = reviews_df['tokens'].values\n",
    "y_labels = reviews_df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokens, y_labels, test_size=0.2, random_state=42, stratify=y_labels)\n",
    "X_train_tensor, y_train_tensor = prepare_data_tensors(X_train, y_train, vocab, max_sequence_length)\n",
    "X_test_tensor, y_test_tensor = prepare_data_tensors(X_test, y_test, vocab, max_sequence_length)\n",
    "batch_size = 64\n",
    "train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size=batch_size, shuffle=False)\n",
    "print(f\"Train loader size: {len(train_loader)}, Test loader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b79da5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "RNNLSTMClassifier(\n",
      "  (embedding): Embedding(32035, 4)\n",
      "  (lstm): LSTM(4, 64, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attention): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      "Total parameters: 172,495\n"
     ]
    }
   ],
   "source": [
    "class RNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                 num_layers=2, dropout=0.3, embedding_matrix=None):\n",
    "        super(RNNLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Use pre-trained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True  # Fine-tune embeddings\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        attended_output = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        attended_output = self.dropout(attended_output)\n",
    "        \n",
    "        output = self.relu(self.fc1(attended_output))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = RNNLSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=64,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7b89ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for _, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy, all_preds, all_targets\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5f5eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.2295, Train Acc: 90.94%\n",
      "Val Loss: 0.1923, Val Acc: 92.61%\n",
      "Learning Rate: 0.001000\n",
      "Epoch 2\n",
      "Train Loss: 0.1830, Train Acc: 92.92%\n",
      "Val Loss: 0.1772, Val Acc: 93.13%\n",
      "Learning Rate: 0.001000\n",
      "Epoch 3\n",
      "Train Loss: 0.1737, Train Acc: 93.27%\n",
      "Val Loss: 0.1695, Val Acc: 93.32%\n",
      "Learning Rate: 0.001000\n",
      "Epoch 4\n",
      "Train Loss: 0.1671, Train Acc: 93.49%\n",
      "Val Loss: 0.1632, Val Acc: 93.62%\n",
      "Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 4\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
